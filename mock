Q1: Add a new worker node to the cluster
Youâ€™ve installed kubeadm and Docker on a new machine.

Join it to the existing cluster using the kubeadm join token.

âœ… Expected Outcome: New node appears in kubectl get nodes

Get the join command from the master node
$ kubeadm token create --print-join-command
Run the join command on the new worker node. Key looks like below as
kubeadm join 10.0.0.1:6443 --token abcdef.0123456789abcdef \
    --discovery-token-ca-cert-hash sha256:1234abcd...efgh5678
$ kubeadm join <control-plane-ip>:6443 --token <token> --discovery-token-ca-cert-hash sha256:<hash> ----> this is sample format of the above command 
On the control plane, verify the node joined:
$ kubectl get no 
Troubleshooting Tips
Firewall: Ensure port 6443 is open from the worker to control-plane.
Clock Sync: Use ntpd or chronyd to avoid TLS errors.
Kubelet logs: Check with journalctl -xeu kubelet if node isn't joining.
journalctl	Command to query system logs
-x	Show extra info (explanations of error codes if available)
-e	Jump to the end of the log (most recent entries)
-u kubelet	Show logs only for the kubelet unit/service
-------------------------------------------------------------------------
 Q2: View the kubelet logs on a node
View logs for the kubelet service on node01.
âœ… Expected: Show the last 20 lines of kubelet logs.
$ journalctl logs -u kubelet -n 20
To watch logs live
$ journalctl logs kubelet -f 
If logs are not showing, make sure kubelet is installing and running
$ systemctl status kubelet

We cannot directly view kubelet logs using kubectl, because:
kubectl interacts with the Kubernetes API server.
kubelet runs on the node itself and is not part of the Kubernetes API.
kubelet logs are system-level logs, not pod logs.
---------------------------------------------------------------------------
Workloads & Scheduling
ðŸ§© Q3: Create a pod that runs on a specific node
Pod name: myapp-pod
Image: nginx
Node: node01
âœ… Expected: Use nodeName or nodeSelector
Using nodeSelector:
$ kubectl get nodes --show-labels
$ kubectl label node node01 disktype=ssd
pod.yaml 
apiVersion: v1
kind: Pod
metadata: 
  name: myapp-pod
spec:
  containers:
    - name: nginx
      image: nginx
  nodeSelector:
    disktype: ssd
Using Node:
myapp-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
spec:
  containers:
    - name: nginx
      image: nginx
  nodeName: node01
$ kubectl apply -f myapp-pod.yaml
$ kubectl get pods -o wide
-----------------------------------------------
Q4: Create a deployment with resource limits
Name: cpu-demo
Image: nginx
Replicas: 2
CPU request: 100m, limit: 500m
Memory request: 256Mi, limit: 512Mi

cpu-demo.yaml\
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cpu-demo
spec:
  replicas: 2
  selector:
    matchLabels:
      app: cpu-demo
  template:
    metadata:
      labels:
        app: cpu-demo
    spec:
      containers:
        - name: nginx
          image: nginx
          resources:
            requests:
              cpu: "100m"
              memory: "256Mi"
            limits:
              cpu: "500m"
              memory: "512Mi"

$ kubectl apply -f cpu-demo.yaml
$ kubectl get deployment cpu-demo 
$ kubectl describe deployment cpu-demo 
---------------------------------------------------------------
 Q5: Schedule a pod with a node affinity
Pod must run on a node with label zone=us-east1
Pod name: affinity-pod, image: busybox

$ kubectl get nodes --show-labels
$ kubectl label node node01 disktype-         # kubectl label node <node-name> <label-key>-
$ kubectl describe node node01 | grep Labels -A 20     #to check labels on a specific node:
$ kubectl get nodes --show-labels
$ kubectl label node node01 zone=us-east1
affinity-pod.yaml
apiVersion: v1
kind: Pod
metadata: 
  name: affinity-pod
spec:
  containers:
    - name: busybox
      image: busybox
  affinity:
    nodeAffinity:
      requireDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: zone
                operator: Equal
                values:
                  - us-east1
$ kubectl apply -f affinity-pod.yaml
$ kubectl get pod affinity-pod -o wide
-------------------------------------------------------
Access Control (RBAC)
ðŸ§© Q6: Create a service account and restrict access
Create service account: dev-user
Give read-only access to pods in namespace dev
âœ… Expected: Use Role, RoleBinding
Create service account:
sa.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: dev-user
  namespace: dev

$ kubectl apply -f sa.yaml 

2. Create a Role with Read Access to Pods
role.yaml
apiVersion: rbac.authentication.k8s.io/v1
kind: Role
metadata:
  namespace: dev
  name: pod-reader
rules:
  - apiGroups: [""]
    resources: ["pods"]
    verbs: ["get", "list", "watch"]
$ kubectl apply -f role.yaml

3. Create a RoleBinding to Bind Role to the ServiceAccount
rolebinding.yaml
apiVersion: rbac.authentication.k8s.io/v1
kind: RoleBinding
metadata:
  name: read-pods-binding
  namespace: dev
subjects: 
  - kind: ServiceAccount
    name: dev-user
    namespace: dev
roleRef:
  kind: Role
  name: pod-reader
  apiGroup: rbac.authentication.k8s.io
$ kubectl apply -f rolebinding.yaml

we can test access using kubectl auth can-i with the service account's token, or by running a pod with that service account.
-------------------------------------------------------------------------------
Storage
ðŸ§© Q7: Create a PVC and mount it
PVC name: my-pvc
Size: 1Gi
Mount to a pod at /data
Use storage class standard
âœ… Expected: PVC is bound and pod can write to /data

my-pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
storageClassName: standard
$ kubectl apply -f my-pvc.yaml 
  
pvc-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: pvc-pod
spec:
  containers:
    - name: busybox
      image: busybox
      command: [ "sleep", "3600" ]
      volumeMounts:
        - name: my-storage
          mountPath: /data
  volumes:
    - name: my-storage
      persistentVolumeClaim:
        claimName: my-pvc

$ kubectl apply -f pvc-pod.yaml 
Check if PVC is bound
$ kubectl get pvc my-pvc
$ kubectl get pod pvc-pod
Enter the pod and write a file:
$ kubectl exec -it pvc-pod -- sh 
echo "test file" > /data/test.txt
cat test.txt 
--------------------------------------------------------
Networking
ðŸ§© Q8: Create a NetworkPolicy
Only allow pods with label role=api to talk to pods with label app=backend in namespace default.
âœ… Expected: Other pods are blocked from accessing app=backend

 network-policy.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-api-to-backend
spec:
  podSelector:
    matchLabels:
      app: backend     #target backend pods
  policyTypes:
    - Ingress
  ingress:
    - from:
        - podSelector:
            matchLabels:
              role: api        # allowed: only with pods role=api 
---------------------------------------------------------------
Cluster Maintenance
ðŸ§© Q9: Drain a node for maintenance
Node: node01
Evict all pods safely
âœ… Expected: Node shows as SchedulingDisabled

$ kubectl get nodes
# $ kubectl cordon node <node-name>
$ kubectl drain <node-name> --ignore-daemonsets --delete-emptydir-data
$ kubectl get nodes
$ kubectl uncordon <node-name> 
-----------------------------------------------------------------
 Security
ðŸ§© Q10: Create a pod security policy (PSP) equivalent
Enforce that containers must run as non-root user.
Apply to pods in namespace secure.
âœ… Expected: Use PodSecurityAdmission with restricted mode (K8s >= v1.25)

Enforce that pods in the secure namespace must run as non-root.
1: Label the namespace with restricted mode
$ kubectl label namespace secure pod-security.kubernetes.io/enforce=restricted
Optional (but recommended) to also audit and warn:
$ kubectl label namespace secure pod-security.kubernetes.io/audi=restricted
$ kubectl label namespace secure pod-security.kubernetes.io/warn=restricted
 What "restricted" mode enforces:
Containers must not run as root.
Must have runAsNonRoot: true or a non-zero runAsUser.
No privilege escalation.
Limited volume types.
Seccomp and capabilities restrictions.
pod.yaml that will pass
apiVersion: v1
kind: Pod
metadata:
  name: secure-po
  namespace: secure
spec:
  containers:
    - name: nginx
      image: nginx
      ports:
        - containerPort: 80
      securityContext: 
        runAsNonRoot: true
        runAsUser: 1000
Pod that will fail
apiVersion: v1
kind: Pod
metadata:
  name: bad-pod
  namespace: secure
spec:
  containers:
    - name: nginx
      image: nginx
----------------------------------------------------------
Logging & Monitoring
ðŸ§© Q11: View logs of a failed pod
Pod name: crashy
Find why it's crashing
âœ… Expected: Use kubectl describe and kubectl logs

$ kubectl describe pod crashy
$ kubectl logs crashy
If the pod has multiple containers, use:
$ kubectl logs crashy -c <container-name> 
If the pod restarts quickly, you may want to see logs from the previous crash:
$ kubectl logs crashy --previous
$ kubectl get pod crashy -o wide
-------------------------------------------------------------
etcd Backup and Restore
ðŸ§© Q12: Backup etcd
Save snapshot to /backup/etcd-snap.db
âœ… Expected: Use etcdctl snapshot save with correct flags
1: Set environment variables (required for etcdctl)
$ export ETCDCTL_API=3    
 2: Run the snapshot command: For a locally running control-plane node, run:
$ etcdctl snapshot save /backup/etcd-snap.db \
    --endpoints=https://127.0.0.1:2379 \
    --cacert=/etc/kubernetes/pki/etcd/ca.crt \
    --cert=/etc/kubernetes/pki/etcd/server.crt \
    --key=/etc/kubernetes/pki/etcd/server.key \

Make sure: /backup/ directory exists:
$ mkdir -p /backup 
Run this on the control-plane node where etcd is running.
-------------------------------------------------------
Troubleshooting
ðŸ§© Q13: Fix a CrashLoopBackOff pod
Pod my-web is crashing due to a missing config
Fix by adding an env variable ENV=prod
âœ… Expected: Use kubectl edit pod or re-apply YAML

Option 1: Quick fix with kubectl edit (live edit)
$ kubectl edit pod my-web
Then inside the YAML, under spec.containers[0], add:
env:
  - name: ENV
    value: "prod"
Note: Directly editing a pod only works temporarily because pods are not meant to be edited live. It's better to fix the Deployment, StatefulSet, or source YAML if possible.
2. Recreate the pod from YAML (recommended if managed by Deployment)
Export the YAML:
$ kubectl get pod my-web -o yaml > my-web.yaml
Edit my-web.yaml and add:
env:
  - name: ENV
    value: "prod"
Delete old pod and reapply:
$ kubectl delete po my-web
$ kubectl apply -f my-web.yaml 
After the fix, confirm the pod is running:
$ kubectl get po
$ kubectl logs my-web 
-----------------------------------------------------
ConfigMaps and Secrets
ðŸ§© Q14: Use a ConfigMap and Secret in a pod
ConfigMap: app-config with APP_MODE=debug
Secret: db-secret with username/password
Mount both in the pod my-app

# configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
data:
  APP_MODE: debug
$ kubectl apply -f configmap.yaml

# secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: db-secret
type: Opaque
stringData:
  username: admin
  password: s3cr3t
$kubectl apply -f secret.yaml

# my-app.yaml
apiVersion: v1
kind: Pod
metadata:
  name: my-app
spec:
  containers:
    - name: app-container
      image: busybox
      command: ['sh', '-c', 'env && sleep 3600']
      env:
        - name: APP_MODE
          valueFrom:
            configMapKeyRef:
              name: app-config
              key: APP_MODE
        - name: DB_USER
          valueFrom:
            secretKeyRef:
              name: db-secret
              key: username
        - name: DB_PASS
          valueFrom:
            secretKeyRef:
              name: db-secret
              key: password
$ kubectl apply -f my-app.yaml
To check if the environment variables were set inside the pod:
$ kubectl exec my-app -- env | grep -E 'APP_MODE|DB_'
You should see:
APP_MODE=debug
DB_USER=admin
DB_PASS=s3cr3t










--------------------------------------------------------














